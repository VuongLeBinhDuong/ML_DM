{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nhm-0MlnAi0",
        "outputId": "a53c7804-8d9d-4745-f995-2195651aeef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 64\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EARLY_STOPPING_PATIENCE = 7\n",
        "EARLY_STOPPING_DELTA = 0.001\n",
        "LR_PATIENCE = 2  # Number of epochs to wait before reducing learning rate\n",
        "LR_FACTOR = 0.5  # Factor to reduce learning rate by\n",
        "MIN_LR = 1e-6  # Minimum learning rate\n",
        "\n",
        "# Custom Dataset with pre-computed BERT outputs\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len, bert_outputs_dir):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        # Load pre-computed BERT outputs\n",
        "        self.bert_logits = np.load(os.path.join(bert_outputs_dir, 'bert_logits.npy'))\n",
        "        self.bert_features = np.load(os.path.join(bert_outputs_dir, 'bert_features.npy'))\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        # Encode text using BERT tokenizer\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        lstm_input = encoding['input_ids'].squeeze(0)  # shape: (max_len,)\n",
        "        # Get pre-computed BERT outputs\n",
        "        bert_logits = torch.tensor(self.bert_logits[idx], dtype=torch.float)\n",
        "        bert_features = torch.tensor(self.bert_features[idx], dtype=torch.float)\n",
        "        return {\n",
        "            'lstm_input': lstm_input,\n",
        "            'bert_logits': bert_logits,\n",
        "            'bert_features': bert_features,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Student Model (LSTM)\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, tokenizer):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=tokenizer.pad_token_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.match_hidden = nn.Linear(hidden_dim * 2, 768)  # Match với BERT\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        # Use mean pooling of all hidden states\n",
        "        last_hidden = torch.mean(lstm_out, dim=1)  # Take mean across sequence length dimension\n",
        "        last_hidden = self.dropout(last_hidden)\n",
        "        matched_hidden = self.match_hidden(last_hidden)  # Đưa về 768 chiều\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits, matched_hidden\n",
        "\n",
        "# Distillation Loss\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, temperature=2.0):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n",
        "        # Soft targets loss\n",
        "        soft_targets = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
        "        soft_prob = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
        "        soft_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size(0)\n",
        "\n",
        "        # Hard targets loss\n",
        "        hard_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Feature-based loss\n",
        "        feature_loss = self.mse_loss(student_features, teacher_features)\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss + 0.1 * feature_loss\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "id": "5dpBfRrDnIEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/ML_DM/final_news_test.csv')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create test dataset (update bert_outputs_dir if needed)\n",
        "test_dataset = TextDataset(\n",
        "    texts=test_df['text'].values,\n",
        "    labels=test_df['label'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    bert_outputs_dir='/content/drive/MyDrive/ML_DM/precomputed_bert/test'\n",
        ")\n",
        "\n",
        "# Create test dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Initialize model\n",
        "student_model = StudentModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embedding_dim=256,\n",
        "    hidden_dim=256,\n",
        "    num_classes=4,\n",
        "    tokenizer=tokenizer\n",
        ").to(DEVICE)\n",
        "\n",
        "# Load best model weights\n",
        "checkpoint = torch.load('/content/drive/MyDrive/ML_DM/Student model/acc 87/best_student_model.pth', map_location=DEVICE)\n",
        "student_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Initialize loss (for feature distillation, not used for accuracy)\n",
        "criterion = DistillationLoss(alpha=0.5, temperature=2.0)\n",
        "\n",
        "# Evaluate\n",
        "student_model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        lstm_input = batch['lstm_input'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "        student_logits, _ = student_model(lstm_input)\n",
        "        preds = torch.argmax(student_logits, dim=1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({sum([p==t for p, t in zip(all_preds, all_labels)])}/{len(all_labels)})\")\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Print results\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(cm)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig('student_confusion_matrix.png')\n",
        "plt.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVoP4hQypMhE",
        "outputId": "e092ba37-0949-4db8-e530-708623a87bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8700 (17048/19596)\n",
            "Test Accuracy: 87.00%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8790    0.8977    0.8882      7026\n",
            "           1     0.8935    0.8848    0.8891      3905\n",
            "           2     0.8730    0.8752    0.8741      5952\n",
            "           3     0.8029    0.7656    0.7838      2713\n",
            "\n",
            "    accuracy                         0.8700     19596\n",
            "   macro avg     0.8621    0.8558    0.8588     19596\n",
            "weighted avg     0.8695    0.8700    0.8696     19596\n",
            "\n",
            "Confusion Matrix:\n",
            "[[6307  185  342  192]\n",
            " [ 256 3455  148   46]\n",
            " [ 324  147 5209  272]\n",
            " [ 288   80  268 2077]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "h-rOSR6drai8",
        "outputId": "9d47ff21-2705-4049-c1a0-614c46457b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4db0ae45f48f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTest Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
          ]
        }
      ]
    }
  ]
}