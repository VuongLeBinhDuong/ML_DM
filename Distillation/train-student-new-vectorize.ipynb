{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11917960,"sourceType":"datasetVersion","datasetId":7492333},{"sourceId":11928640,"sourceType":"datasetVersion","datasetId":7499421}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom collections import Counter\nimport os\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_LEN = 64\nBATCH_SIZE = 16\nEPOCHS = 50\nLEARNING_RATE = 2e-5\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nEARLY_STOPPING_PATIENCE = 7\nEARLY_STOPPING_DELTA = 0.001\nLR_PATIENCE = 2  # Number of epochs to wait before reducing learning rate\nLR_FACTOR = 0.5  # Factor to reduce learning rate by\nMIN_LR = 1e-6  # Minimum learning rate\n\n# Custom Dataset with pre-computed BERT outputs\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len, bert_outputs_dir):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        # Load pre-computed BERT outputs\n        self.bert_logits = np.load(os.path.join(bert_outputs_dir, 'bert_logits.npy'))\n        self.bert_features = np.load(os.path.join(bert_outputs_dir, 'bert_features.npy'))\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        # Encode text using BERT tokenizer\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=False,\n            return_tensors='pt'\n        )\n        lstm_input = encoding['input_ids'].squeeze(0)  # shape: (max_len,)\n        # Get pre-computed BERT outputs\n        bert_logits = torch.tensor(self.bert_logits[idx], dtype=torch.float)\n        bert_features = torch.tensor(self.bert_features[idx], dtype=torch.float)\n        return {\n            'lstm_input': lstm_input,\n            'bert_logits': bert_logits,\n            'bert_features': bert_features,\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Student Model (LSTM)\nclass StudentModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n        super(StudentModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.dropout = nn.Dropout(0.25)\n        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n        self.match_hidden = nn.Linear(hidden_dim * 2, 768)  # Match với BERT\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, _ = self.lstm(embedded)\n        # Use mean pooling of all hidden states\n        last_hidden = torch.mean(lstm_out, dim=1)  # Take mean across sequence length dimension\n        last_hidden = self.dropout(last_hidden)\n        matched_hidden = self.match_hidden(last_hidden)  # Đưa về 768 chiều\n        logits = self.classifier(last_hidden)\n        return logits, matched_hidden\n\n# Distillation Loss\nclass DistillationLoss(nn.Module):\n    def __init__(self, alpha=0.5, temperature=2.0):\n        super(DistillationLoss, self).__init__()\n        self.alpha = alpha\n        self.temperature = temperature\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.mse_loss = nn.MSELoss()\n        \n    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n        # Soft targets loss\n        soft_targets = F.softmax(teacher_logits / self.temperature, dim=-1)\n        soft_prob = F.log_softmax(student_logits / self.temperature, dim=-1)\n        soft_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size(0)\n        \n        # Hard targets loss\n        hard_loss = self.ce_loss(student_logits, labels)\n        \n        # Feature-based loss\n        feature_loss = self.mse_loss(student_features, teacher_features)\n        \n        # Combine losses\n        total_loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss + 0.1 * feature_loss\n        return total_loss\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:49:02.933660Z","iopub.execute_input":"2025-05-25T01:49:02.933953Z","iopub.status.idle":"2025-05-25T01:49:09.913707Z","shell.execute_reply.started":"2025-05-25T01:49:02.933930Z","shell.execute_reply":"2025-05-25T01:49:09.912888Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def train_model(student_model, train_loader, optimizer, criterion, device):\n    student_model.train()\n    total_loss = 0\n    \n    for batch in tqdm(train_loader):\n        lstm_input = batch['lstm_input'].to(device)\n        teacher_logits = batch['bert_logits'].to(device)\n        teacher_features = batch['bert_features'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Get student outputs\n        student_logits, student_features = student_model(lstm_input)\n        \n        # Calculate loss\n        loss = criterion(student_logits, teacher_logits, student_features, teacher_features, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Clear memory\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    return total_loss / len(train_loader)\n\ndef evaluate_model(student_model, data_loader, criterion, device):\n    student_model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            lstm_input = batch['lstm_input'].to(device)\n            teacher_logits = batch['bert_logits'].to(device)\n            teacher_features = batch['bert_features'].to(device)\n            labels = batch['label'].to(device)\n            \n            student_logits, student_features = student_model(lstm_input)\n            loss = criterion(student_logits, teacher_logits, student_features, teacher_features, labels)\n            total_loss += loss.item()\n            \n            # Clear memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                \n    return total_loss / len(data_loader)\n\ndef test_model(student_model, test_loader, criterion, device):\n    student_model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            lstm_input = batch['lstm_input'].to(device)\n            teacher_logits = batch['bert_logits'].to(device)\n            teacher_features = batch['bert_features'].to(device)\n            labels = batch['label'].to(device)\n            \n            student_logits, student_features = student_model(lstm_input)\n            loss = criterion(student_logits, teacher_logits, student_features, teacher_features, labels)\n            total_loss += loss.item()\n            \n            preds = torch.argmax(student_logits, dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            \n            # Clear memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                \n    avg_loss = total_loss / len(test_loader)\n    accuracy = correct / total\n    cm = confusion_matrix(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, digits=4)\n    return avg_loss, accuracy, cm, report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:49:09.914927Z","iopub.execute_input":"2025-05-25T01:49:09.915698Z","iopub.status.idle":"2025-05-25T01:49:09.926101Z","shell.execute_reply.started":"2025-05-25T01:49:09.915678Z","shell.execute_reply":"2025-05-25T01:49:09.925379Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, scheduler, epoch, best_metric, filename):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n        'epoch': epoch,\n        'best_metric': best_metric\n    }\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(model, optimizer, scheduler, filename):\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    if checkpoint['scheduler_state_dict'] and scheduler:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    return checkpoint['epoch'], checkpoint['best_metric']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:49:09.926895Z","iopub.execute_input":"2025-05-25T01:49:09.927150Z","iopub.status.idle":"2025-05-25T01:49:09.952049Z","shell.execute_reply.started":"2025-05-25T01:49:09.927126Z","shell.execute_reply":"2025-05-25T01:49:09.951481Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# Load and preprocess data\ntrain_df = pd.read_csv('/kaggle/input/news-dataset/final_news_train.csv')\ntest_df = pd.read_csv('/kaggle/input/news-dataset/final_news_test.csv')\n# Split train into train and validation\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df['text'].values,\n    train_df['label'].values,\n    test_size=0.1,\n    random_state=42,\n    stratify=train_df['label'].values\n)\n# Initialize tokenizer\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# Create datasets with pre-computed BERT outputs\ntrain_dataset = TextDataset(\n    texts=train_texts,\n    labels=train_labels,\n    tokenizer=bert_tokenizer,\n    max_len=MAX_LEN,\n    bert_outputs_dir='/kaggle/input/precomputed-bert/precomputed_bert/train'\n)\nval_dataset = TextDataset(\n    texts=val_texts,\n    labels=val_labels,\n    tokenizer=bert_tokenizer,\n    max_len=MAX_LEN,\n    bert_outputs_dir='/kaggle/input/precomputed-bert/precomputed_bert/val'  # Use separate validation outputs\n)\ntest_dataset = TextDataset(\n    texts=test_df['text'].values,\n    labels=test_df['label'].values,\n    tokenizer=bert_tokenizer,\n    max_len=MAX_LEN,\n    bert_outputs_dir='/kaggle/input/precomputed-bert/precomputed_bert/test'\n)\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n# Initialize student model\nstudent_model = StudentModel(\n    vocab_size=bert_tokenizer.vocab_size,\n    embedding_dim=256,\n    hidden_dim=256,\n    num_classes=4\n).to(DEVICE)\n\n# Initialize optimizer and criterion\noptimizer = torch.optim.Adam(student_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\ncriterion = DistillationLoss(alpha=0.5, temperature=2.0)\n\n# Initialize learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=LR_FACTOR,\n    patience=LR_PATIENCE,\n    min_lr=MIN_LR,\n    verbose=True\n)\n\n# Always start training from scratch\nstart_epoch = 0\nbest_val_loss = float('inf')\ntrain_losses = []\nval_losses = []\n\n# Training loop\nfor epoch in range(start_epoch, EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    # Clear GPU cache at the start of each epoch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    train_loss = train_model(student_model, train_loader, optimizer, criterion, DEVICE)\n    val_loss = evaluate_model(student_model, val_loader, criterion, DEVICE)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    print(f'Training Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}')\n    # Update learning rate based on validation loss\n    scheduler.step(val_loss)\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f'Current Learning Rate: {current_lr:.2e}')\n    # Early stopping check\n    if val_loss < best_val_loss - EARLY_STOPPING_DELTA:\n        patience_counter = 0\n        best_val_loss = val_loss\n        # Save best model\n        save_checkpoint(\n            student_model,\n            optimizer,\n            scheduler,\n            epoch,\n            best_val_loss,\n            'best_student_model.pth'\n        )\n        print('Best model saved!')\n    else:\n            patience_counter += 1\n            print(f'EarlyStopping counter: {patience_counter} out of {EARLY_STOPPING_PATIENCE}')\n            if patience_counter >= EARLY_STOPPING_PATIENCE:\n                print('Early stopping triggered')\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T01:49:09.953365Z","iopub.execute_input":"2025-05-25T01:49:09.953630Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c35c653f32469a8c14bc95f80f2b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dcdfdf18b4d4c8e913d940a7afecdd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e09efe26cc754831b71611bd56d2ca2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87bd5f0187cb41af9bb1ef73229e2bad"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.0830 | Validation Loss: 0.8992\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.8102 | Validation Loss: 0.7659\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7211 | Validation Loss: 0.7321\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6773 | Validation Loss: 0.7194\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6499 | Validation Loss: 0.6864\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6283 | Validation Loss: 0.6507\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6092 | Validation Loss: 0.6337\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5920 | Validation Loss: 0.6433\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 1 out of 7\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5749 | Validation Loss: 0.6099\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5595 | Validation Loss: 0.5915\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5456 | Validation Loss: 0.5887\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5341 | Validation Loss: 0.5741\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5234 | Validation Loss: 0.5730\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5145 | Validation Loss: 0.5639\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5063 | Validation Loss: 0.5592\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4995 | Validation Loss: 0.5617\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 1 out of 7\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4935 | Validation Loss: 0.5538\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4880 | Validation Loss: 0.5525\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4831 | Validation Loss: 0.5490\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4787 | Validation Loss: 0.5552\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 1 out of 7\nEpoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4743 | Validation Loss: 0.5489\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 2 out of 7\nEpoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4710 | Validation Loss: 0.5526\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 3 out of 7\nEpoch 23/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:57<00:00, 72.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4679 | Validation Loss: 0.5423\nCurrent Learning Rate: 2.00e-05\nBest model saved!\nEpoch 24/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4645 | Validation Loss: 0.5464\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 1 out of 7\nEpoch 25/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4618 | Validation Loss: 0.5466\nCurrent Learning Rate: 2.00e-05\nEarlyStopping counter: 2 out of 7\nEpoch 26/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4592 | Validation Loss: 0.5459\nCurrent Learning Rate: 1.00e-05\nEarlyStopping counter: 3 out of 7\nEpoch 27/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4416 | Validation Loss: 0.5539\nCurrent Learning Rate: 1.00e-05\nEarlyStopping counter: 4 out of 7\nEpoch 28/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4387 | Validation Loss: 0.5394\nCurrent Learning Rate: 1.00e-05\nBest model saved!\nEpoch 29/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4365 | Validation Loss: 0.5510\nCurrent Learning Rate: 1.00e-05\nEarlyStopping counter: 1 out of 7\nEpoch 30/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 72.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4347 | Validation Loss: 0.5569\nCurrent Learning Rate: 1.00e-05\nEarlyStopping counter: 2 out of 7\nEpoch 31/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 71.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4334 | Validation Loss: 0.5603\nCurrent Learning Rate: 5.00e-06\nEarlyStopping counter: 3 out of 7\nEpoch 32/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12823/12823 [02:58<00:00, 72.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4226 | Validation Loss: 0.5486\nCurrent Learning Rate: 5.00e-06\nEarlyStopping counter: 4 out of 7\nEpoch 33/50\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 11025/12823 [02:33<00:24, 72.71it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Sau khi kết thúc training, plot và lưu hình:\nplt.figure(figsize=(8, 5))\nplt.plot(range(start_epoch + 1, start_epoch + 1 + len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(start_epoch + 1, start_epoch + 1 + len(val_losses)), val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train/Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nplt.close()\n\n# Đánh giá trên test set với best model\nprint('Evaluating on test set with best model...')\nstudent_model.load_state_dict(torch.load('best_student_model.pth')['model_state_dict'])\ntest_loss, test_acc, cm, report = test_model(student_model, test_loader, criterion, DEVICE)\nprint(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}')\nprint('Confusion Matrix:')\nprint(cm)\nprint('Classification Report:')\nprint(report)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}