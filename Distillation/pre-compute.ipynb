{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11917960,"sourceType":"datasetVersion","datasetId":7492333},{"sourceId":409456,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":334515,"modelId":355548},{"sourceId":408401,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":333672,"modelId":354670}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 16\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(DEVICE)\n\n# Custom Dataset\nclass BertDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Teacher Model (BERT)\nclass TeacherModel(nn.Module):\n    def __init__(self, num_classes):\n        super(TeacherModel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        pooled_output = outputs[1]  # [CLS] token representation\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits, pooled_output\n\ndef precompute_bert_outputs(model, data_loader, device, save_dir):\n    model.eval()\n    all_logits = []\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc='Precomputing BERT outputs'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            logits, features = model(input_ids, attention_mask)\n            \n            all_logits.append(logits.cpu().numpy())\n            all_features.append(features.cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n            \n            # Clear GPU memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    \n    # Concatenate all batches\n    all_logits = np.concatenate(all_logits, axis=0)\n    all_features = np.concatenate(all_features, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    \n    # Create save directory if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Save precomputed outputs\n    np.save(os.path.join(save_dir, 'bert_logits.npy'), all_logits)\n    np.save(os.path.join(save_dir, 'bert_features.npy'), all_features)\n    np.save(os.path.join(save_dir, 'labels.npy'), all_labels)\n    \n    print(f\"Saved precomputed outputs to {save_dir}\")\n    print(f\"Logits shape: {all_logits.shape}\")\n    print(f\"Features shape: {all_features.shape}\")\n    print(f\"Labels shape: {all_labels.shape}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:43:20.902299Z","iopub.execute_input":"2025-05-24T03:43:20.902622Z","iopub.status.idle":"2025-05-24T03:43:20.916393Z","shell.execute_reply.started":"2025-05-24T03:43:20.902593Z","shell.execute_reply":"2025-05-24T03:43:20.915698Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load data\ntrain_df = pd.read_csv('/kaggle/input/news-dataset/final_news_train.csv')\ntest_df = pd.read_csv('/kaggle/input/news-dataset/final_news_test.csv')\n\n# Split train into train and validation\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df['text'].values,\n    train_df['label'].values,\n    test_size=0.1,\n    random_state=42,\n    stratify=train_df['label'].values\n)\n\n# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create datasets\ntrain_dataset = BertDataset(\n    texts=train_texts,\n    labels=train_labels,\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\nval_dataset = BertDataset(\n    texts=val_texts,\n    labels=val_labels,\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\ntest_dataset = BertDataset(\n    texts=test_df['text'].values,\n    labels=test_df['label'].values,\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n\n# Initialize teacher model\nteacher_model = TeacherModel(num_classes=4).to(DEVICE)\n\n# Load pre-trained BERT weights\nteacher_model.load_state_dict(torch.load('/kaggle/input/bert_classifier/pytorch/default/1/bert_classifier.pth'))\nprint(\"Teacher model loaded successfully!\")\n\n# Precompute and save BERT outputs for training data\nprint(\"\\nPrecomputing BERT outputs for training data...\")\nprecompute_bert_outputs(teacher_model, train_loader, DEVICE, '/kaggle/working/precomputed_bert/train')\n\n# Precompute and save BERT outputs for validation data\nprint(\"\\nPrecomputing BERT outputs for validation data...\")\nprecompute_bert_outputs(teacher_model, val_loader, DEVICE, '/kaggle/working/precomputed_bert/val')\n\n# Precompute and save BERT outputs for test data\nprint(\"\\nPrecomputing BERT outputs for test data...\")\nprecompute_bert_outputs(teacher_model, test_loader, DEVICE, '/kaggle/working/precomputed_bert/test')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:43:20.917425Z","iopub.execute_input":"2025-05-24T03:43:20.917620Z","iopub.status.idle":"2025-05-24T04:00:15.947314Z","shell.execute_reply.started":"2025-05-24T03:43:20.917598Z","shell.execute_reply":"2025-05-24T04:00:15.946116Z"}},"outputs":[{"name":"stdout","text":"Teacher model loaded successfully!\n\nPrecomputing BERT outputs for training data...\n","output_type":"stream"},{"name":"stderr","text":"Precomputing BERT outputs:  60%|██████    | 7731/12823 [16:53<11:07,  7.63it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_104/498021304.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Precompute and save BERT outputs for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPrecomputing BERT outputs for training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mprecompute_bert_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/kaggle/working/precomputed_bert/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Precompute and save BERT outputs for validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_104/289825057.py\u001b[0m in \u001b[0;36mprecompute_bert_outputs\u001b[0;34m(model, data_loader, device, save_dir)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mall_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"!zip -r /kaggle/working/output.zip /kaggle/working/precomputed_bert","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T04:00:31.374688Z","iopub.execute_input":"2025-05-24T04:00:31.375448Z","iopub.status.idle":"2025-05-24T04:01:14.044647Z","shell.execute_reply.started":"2025-05-24T04:00:31.375418Z","shell.execute_reply":"2025-05-24T04:01:14.043904Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/precomputed_bert/ (stored 0%)\n  adding: kaggle/working/precomputed_bert/test/ (stored 0%)\n  adding: kaggle/working/precomputed_bert/test/bert_logits.npy (deflated 8%)\n  adding: kaggle/working/precomputed_bert/test/bert_features.npy (deflated 8%)\n  adding: kaggle/working/precomputed_bert/test/labels.npy (deflated 94%)\n  adding: kaggle/working/precomputed_bert/train/ (stored 0%)\n  adding: kaggle/working/precomputed_bert/train/bert_logits.npy (deflated 8%)\n  adding: kaggle/working/precomputed_bert/train/bert_features.npy (deflated 8%)\n  adding: kaggle/working/precomputed_bert/train/labels.npy (deflated 94%)\n  adding: kaggle/working/precomputed_bert/val/ (stored 0%)\n  adding: kaggle/working/precomputed_bert/val/bert_logits.npy (deflated 8%)\n  adding: kaggle/working/precomputed_bert/val/bert_features.npy (deflated 8%)\n  adding: kaggle/working/precomputed_bert/val/labels.npy (deflated 94%)\n","output_type":"stream"}],"execution_count":6}]}